<!doctype html>
<html lang="">	
<head>
	<meta charset="utf-8"/>
	<title>Strong Inference</title>	
	<meta name="author" content="Christopher Fonnesbeck">
	

	<link rel="stylesheet" href="http://stronginference.com/theme/css/main.css" type="text/css" />
		


    <link href="http://stronginference.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Strong Inference Atom Feed" />
</head>
	
<body>

    <div class="container">
	  
	  <header role="banner">
	    <div class="feeds">
	        <a href="http://stronginference.com/feeds/all.atom.xml" rel="alternate"><img src="/theme/images/icons/feed-32px.png" alt="atom feed"/></a>
	    </div>
		<a href="http://stronginference.com" class="title">Strong Inference</a>
      </header>
	
	  <div class="wrapper">

		  <div role="main" class="content">



      <article>
        <h1><a href="http://stronginference.com/bayes-factors-pymc.html">Calculating Bayes factors with PyMC</a></h1>
        <script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<p>Statisticians are sometimes interested in comparing two (or more) models, with respect to their relative support by a particular dataset. This may be in order to select the best model to use for inference, or to weight models so that they can be averaged for use in multimodel inference. </p>
<p>The <a href="http://en.wikipedia.org/wiki/Bayes_factor">Bayes factor</a> is a good choice when comparing two arbitrary models, and the parameters of those models have been estimated. Bayes factors are simply ratios of <em>marginal</em> likelihoods for competing models:</p>
<p>$$ \text{BF}_{i,j} = \frac{L(Y \mid M_i)}{L(Y \mid M_j)} = \frac{\int L(Y \mid M_i,\theta_i)p(\mid \theta_i \mid M_i)d\theta}{\int L(Y \mid M_j,\theta_j)p(\theta_j \mid M_j)d\theta} $$</p>
<p>While passingly similar to likelihood ratios, Bayes factors are calculated using likelihoods that have been integrated with respect to the unknown parameters. In contrast, likelihood ratios are calculated based on the maximum likelihood values of the parameters. This is an important difference, which makes Bayes factors a more effective means of comparing models, since it takes into account parametric uncertainty; likelihood ratios ignore this uncertainty. In addition, unlike likelihood ratios, the two models need not be nested. In other words, one model does not have to be a special case of the other.</p>
<p>Bayes factors are called Bayes factors because they are used in a Bayesian context by updating prior odds with information from data.</p>
<blockquote>
<p>Posterior odds = Bayes factor x Prior odds</p>
</blockquote>
<p>Hence, they represent the evidence in the data for changing the prior odds of one model over another. It is this interpretation as a measure of evidence that makes the Bayes factor a compelling choice for model selection.</p>
<p>One of the obstacles to the wider use of Bayes factors is the difficulty associated with calculating them. While likelihood ratios can be obtained simply by the use of MLEs for all model parameters, Bayes factors require the integration over all unknown model parameters. Hence, for most interesting models Markov chain Monte Carlo (MCMC) is the easiest way to obtain Bayes factors.</p>
<p>Here's a quick tutorial on how to obtain Bayes factors from <a href="https://github.com/pymc-devs/pymc">PyMC</a>. I'm going to use a simple example taken from Chapter 7 of <a href="http://amzn.to/gGV2rK">Link and Barker (2010)</a>. Consider a short vector of data, consisting of 5 integers:</p>
<div class="highlight"><pre><span class="n">Y</span> <span class="o">=</span> <span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">8</span><span class="p">])</span>
</pre></div>


<p>We wish to determine which of two functional forms best models this dataset. The first is a <a href="http://en.wikipedia.org/wiki/Geometric_distribution">geometric model</a>:</p>
<p>$$ f(x|p) = (1-p)^x p $$</p>
<p>and the second a <a href="http://en.wikipedia.org/wiki/Poisson_distribution">Poisson model</a>:</p>
<p>$$ f(x|\mu) = \frac{\mu^x e^{-\mu}}{x!} $$</p>
<p>Both describe the distribution of non-negative integer data, but differ in that the variance of Poisson data is equal to the mean, while the geometric model describes variance greater the mean. For this dataset, the sample variance would suggest that the geometric model is favored, but the sample is too small to say so definitively.</p>
<p>In order to calculate Bayes factors, we require both the prior and posterior odds:</p>
<blockquote>
<p>Bayes factor = Posterior odds / Prior odds</p>
</blockquote>
<p>The Bayes factor does not depend on the value of the prior model weights, but the estimate will be most precise when the posterior odds are the same. For our purposes, we will give 0.1 probability to the geometric model, and 0.9 to the Poisson model:</p>
<div class="highlight"><pre><span class="n">pi</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">)</span>
</pre></div>


<p>Next, we need to specify a latent variable, which identifies the true model (we don't believe either model is "true", but we hope one is better than the other). This is easily done using a categorical random variable, that identifies one model or the other, according to their relative weight.</p>
<div class="highlight"><pre><span class="n">true_model</span> <span class="o">=</span> <span class="n">Categorical</span><span class="p">(</span><span class="s">&quot;true_model&quot;</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">pi</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>


<p>Here, we use the specified prior weights as the categorical probabilities, and the variable has been arbitrarily initialized to zero (the geometric model).</p>
<p>Next, we need prior distributions for the parameters of the two models. For the Poisson model, the expected value is given a uniform prior on the interval [0,1000]:</p>
<div class="highlight"><pre><span class="n">mu</span> <span class="o">=</span> <span class="n">Uniform</span><span class="p">(</span><span class="s">&#39;mu&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>


<p>This stochastic node can be used for the geometric model as well, though it needs to be transformed for use with that distribution:</p>
<div class="highlight"><pre><span class="n">p</span> <span class="o">=</span> <span class="n">Lambda</span><span class="p">(</span><span class="s">&#39;p&#39;</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">:</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">mu</span><span class="p">))</span>
</pre></div>


<p>Finally, the data are incorporated by specifying the appropriate likelihood. We require a mixture of geometric and Poisson likelihoods, depending on which value <em>true_model</em> takes. While BUGS requires an obscure trick to implement such a mixture, PyMC allows for the specification of arbitrary stochastic nodes: </p>
<div class="highlight"><pre><span class="nd">@observed</span>
<span class="k">def</span> <span class="nf">Ylike</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">Y</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="n">true_model</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Either Poisson or geometric, depending on M&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">geometric_like</span><span class="p">(</span><span class="n">value</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">M</span><span class="o">==</span><span class="mi">0</span><span class="p">)</span> <span class="ow">or</span> <span class="n">poisson_like</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">mu</span><span class="p">)</span>
</pre></div>


<p>Notice that the function returns the geometric likelihood when M=0, or the Poisson model otherwise. Now, all that remains is to run the model, and extract the posterior quantities to calculate the Bayes factor.</p>
<p>Though we may be interested in the posterior estimate of the mean, all that we care about from a model selection standpoint is the estimate of <em>true_model</em>. At every iteration, the value of this parameter takes the value of zero for the geometric model and one for the Poisson. Hence, the mean (or median) will be an estimate of the probability of the Poisson model: </p>
<div class="highlight"><pre><span class="n">In</span> <span class="p">[</span><span class="mi">11</span><span class="p">]:</span> <span class="n">M</span><span class="o">.</span><span class="n">true_model</span><span class="o">.</span><span class="n">stats</span><span class="p">()[</span><span class="s">&#39;mean&#39;</span><span class="p">]</span>

<span class="n">Out</span><span class="p">[</span><span class="mi">11</span><span class="p">]:</span> <span class="mf">0.39654545454545453</span>
</pre></div>


<p>So, the posterior probability that the Poisson model is true is about 0.4, leaving 0.6 for the geometric model. The Bayes factor in favor of the geometric model is simply:</p>
<div class="highlight"><pre><span class="n">In</span> <span class="p">[</span><span class="mi">18</span><span class="p">]:</span> <span class="n">p_pois</span> <span class="o">=</span> <span class="n">M</span><span class="o">.</span><span class="n">true_model</span><span class="o">.</span><span class="n">stats</span><span class="p">()[</span><span class="s">&#39;mean&#39;</span><span class="p">]</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">19</span><span class="p">]:</span> <span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">p_pois</span><span class="p">)</span><span class="o">/</span><span class="n">p_pois</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mf">0.1</span><span class="o">/</span><span class="mf">0.9</span><span class="p">)</span>

<span class="n">Out</span><span class="p">[</span><span class="mi">19</span><span class="p">]:</span> <span class="mf">13.696011004126548</span>
</pre></div>


<p>This value can be interpreted as strong evidence in favor of the geometric model.</p>
<p>If you want to run the model for yourself, <a href="https://github.com/pymc-devs/pymc/wiki/BayesFactor">you can download the code here</a>.</p>
      </article>

      <hr />

        <div>
          <h3>Last posts</h3>
          <ol class="archive">
    


      <li>
<a href="http://stronginference.com/burn-in-and-other-mcmc-folklore.html" rel="bookmark" title="Permalink to Burn-in, and Other MCMC Folklore">Burn-in, and Other MCMC Folklore</a>
  <time datetime="2014-08-09T00:00:00-05:00" pubdate>Sat 09 August 2014</time>
<p class="tags">tags: <a href="http://stronginference.com/tag/bayesian.html">bayesian</a><a href="http://stronginference.com/tag/mcmc.html">mcmc</a><a href="http://stronginference.com/tag/pymc.html">pymc</a><a href="http://stronginference.com/tag/python.html">python</a></p></a>      </li>


      <li>
<a href="http://stronginference.com/implementing-dirichlet-processes-for-bayesian-semi-parametric-models.html" rel="bookmark" title="Permalink to Implementing Dirichlet processes for Bayesian semi-parametric models">Implementing Dirichlet processes for Bayesian semi-parametric models</a>
  <time datetime="2014-03-07T00:00:00-06:00" pubdate>Fri 07 March 2014</time>
<p class="tags">tags: <a href="http://stronginference.com/tag/bayesian.html">bayesian</a><a href="http://stronginference.com/tag/pymc.html">pymc</a><a href="http://stronginference.com/tag/mcmc.html">mcmc</a><a href="http://stronginference.com/tag/python.html">python</a></p></a>      </li>


      <li>
<a href="http://stronginference.com/missing-data-imputation.html" rel="bookmark" title="Permalink to Automatic Missing Data Imputation with PyMC">Automatic Missing Data Imputation with PyMC</a>
  <time datetime="2013-08-18T00:00:00-05:00" pubdate>Sun 18 August 2013</time>
<p class="tags">tags: <a href="http://stronginference.com/tag/pymc.html">pymc</a><a href="http://stronginference.com/tag/mcmc.html">mcmc</a><a href="http://stronginference.com/tag/python.html">python</a></p></a>      </li>


          </ol>
        </div>

		  </div>	
		  
		  <div class="sidebar">
		    <div class="sidebar-container" >


  	          <nav>
	            <h2>Categories</h2>
	            <ul>
	                <li ><a href="http://stronginference.com/category/statistics.html">Statistics</a></li>
	            </ul>
	          </nav>

	            <aside>
	            <h2>Social</h2>
			      <ul class="social">
				    <li><a href="https://twitter.com/fonnesbeck">Twitter</a><i></i></li>
				    <li><a href="http://github.com/fonnesbeck">Github</a><i></i></li>
			      </ul>
			    </aside>

	            <aside>
	              <h2>Blogroll</h2>
	              <ul>
	                  <li><a href="http://andrewgelman.com/">Statistical Modeling, Causal Inference, and Social Science</a></li>
	                  <li><a href="http://www.johndcook.com/blog/">John D. Cook</a></li>
	                  <li><a href="https://jakevdp.github.io">Pythonic Perambulations</a></li>
	                  <li><a href="http://healthyalgorithms.com">Healthy Algorithms</a></li>
	                  <li><a href="http://glau.ca">Glau.ca</a></li>
	                  <li><a href="http://statsinthewild.com">Stats in the Wild</a></li>
	                  <li><a href="http://simplystatistics.org">SimplyStats</a></li>
	                  <li><a href="http://blog.yhathq.com">Å·hat</a></li>
	              </ul>
	            </aside>
	        </div>
		  </div>

	  </div>

      <footer>
		<p role="contentinfo">
		  Christopher Fonnesbeck - Proudly powered by <a href="http://alexis.notmyidea.org/pelican/">pelican</a>. Theme <a href="https://github.com/fle/pelican-sober">pelican-sober</a>.
    	</p>

	  </footer>	

	</div>
	

</body>
</html>