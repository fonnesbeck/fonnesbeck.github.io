<!doctype html>
<html lang="">	
<head>
	<meta charset="utf-8"/>
	<title>Automatic Missing Data Imputation with PyMC - Strong Inference</title>	
	<meta name="author" content="Christopher Fonnesbeck">
	

  <meta name="description" content="A distinct advantage of using Bayesian inference is in its universal application of probability models for providing inference. As such, all components of a Bayesian model are specified using probability distributions for either describing a ...">

	<meta name="twitter:card" content="summary">
	  <meta name="twitter:creator" content="@fonnesbeck">
	<meta name="twitter:title" content="Automatic Missing Data Imputation with PyMC">
	<meta name="twitter:description" content="A distinct advantage of using Bayesian inference is in its universal application of probability models for providing inference. As such, all components of a Bayesian model are specified using probability distributions for either describing a ...">
	<meta name="twitter:url" content="http://stronginference.com/missing-data-imputation.html">


	<link rel="stylesheet" href="http://stronginference.com/theme/css/main.css" type="text/css" />
		
	  <script type="text/javascript" src="http://stronginference.com/theme/js/custom_scripts.js"></script>


</head>
	
<body>

    <div class="container">
	  
	  <header role="banner">
	    <div class="feeds">
	    </div>
		<a href="http://stronginference.com" class="title">Strong Inference</a>
      </header>
	
	  <div class="wrapper">

		  <div role="main" class="content">
	<article class="full">
		
		<h1>Automatic Missing Data Imputation with PyMC</h1>
		
<div class="metadata">
  <time datetime="2013-08-18T00:00:00-05:00" pubdate>Sun 18 August 2013</time>
    <address class="vcard author">
      by <a class="url fn" href="http://stronginference.com/author/christopher-fonnesbeck.html">Christopher Fonnesbeck</a>
    </address>
  in <a href="http://stronginference.com/category/statistics.html">Statistics</a>
<p class="tags">tagged <a href="http://stronginference.com/tag/pymc.html">pymc</a>, <a href="http://stronginference.com/tag/mcmc.html">mcmc</a>, <a href="http://stronginference.com/tag/python.html">python</a></p></div>		
		<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<p>A distinct advantage of using Bayesian inference is in its universal application of probability models for providing inference. As such, all components of a Bayesian model are specified using probability distributions for either describing a sampling model (in the case of observed data) or characterizing the uncertainty of an unknown quantity. This means that missing data are treated the same as parameters, and so imputation proceeds very much like estimation. When using Markov chain Monte Carlo (MCMC) to fit Bayesian models it usually requires only a few extra lines of code to impute missing values, based on the sampling distribution of the missing data, and associated (usually unknown) parameters. Using <a href="http://github.com/pymc-devs/pymc" title="PyMC on GitHhub">PyMC built from the latest development code</a>, missing data imputation can be done automatically.</p>
<h2>Types of Missing Data</h2>
<p>The appropriate treatment of missing data depends strongly on how the data came to be missing from the dataset. These mechanisms can be broadly classified into three groups, according to how much information and effort is required to deal with them adequately.</p>
<h3>Missing completely at random (MCAR)</h3>
<p>If data are MCAR, then the probability of that any given datum is missing is equal over the whole dataset. In other words, each datum that is present had the same probability of being missing as each datum that is absent. This implies that ignoring the missing data will not bias inference.</p>
<h3>Missing at random (MAR)</h3>
<p>MAR allows for data to be missing according to a random process, but is more general than MCAR in that all units do not have equal probabilities of being missing. The constraint here is that missingness may only depend on information that is fully observed. For example, the reporting of income on surveys may vary according to some measured factor, such as age, race or sex. We can thus account for heterogeneity in the probability of reporting income by controlling for the measured covariate in whatever model is used for infrence.</p>
<h3>Missing not at random (MNAR)</h3>
<p>When the probability of missing data varies according to information that is not available, this is classified as MNAR. This can either be because suitable covariates for explaining missingness have not been recorded (or are otherwise unavailable) or the probability of being missing depends on the value of the missing datum itself. Extending the previous example, if the probability of reporting income varied according to income itself, this is missing not at random.</p>
<p>In each of these situations, the missing data may be imputed using a sampling model, though in the case of missing not at random, it may be difficult to validate the assumptions required to specify such a model. For the purposes of quickly demonstrating automatic imputation in PyMC, I will illustrate using data that is MCAR.</p>
<h2>Implementing imputation in PyMC</h2>
<p>One of the recurring examples in the PyMC documentation is the coal mining disasters dataset from <a href="http://biomet.oxfordjournals.org/cgi/content/short/66/1/191" title="Jarrett RG (1979). A Note on the Intervals Between Coal Mining Disasters. Biometrika, 66, 191â€“193.">Jarrett 1979</a>. This is a simple longitudinal dataset consisting of counts of coal mining disasters in the U.K. between 1851 and 1962. The objective of the analysis is to identify a switch point in the rate of disasters, from a relatively high rate early in the time series to a lower one later on. Hence, we are interested in estimating two rates, in addition to the year after which the rate changed.</p>
<p>In order to illustrate imputation, I have randomly replaced the data for two years with a missing data placeholder value, -999:</p>
<div class="highlight"><pre>disasters_array =   np.array([ 4, 5, 4, 0, 1, 4, 3, 4, 0, 6, 3, 3, 4, 0, 2, 6,
                   3, 3, 5, 4, 5, 3, 1, 4, 4, 1, 5, 5, 3, 4, 2, 5,
                   2, 2, 3, 4, 2, 1, 3, -999, 2, 1, 1, 1, 1, 3, 0, 0,
                   1, 0, 1, 1, 0, 0, 3, 1, 0, 3, 2, 2, 0, 1, 1, 1,
                   0, 1, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 1, 1, 0, 2,
                   3, 3, 1, -999, 2, 1, 1, 1, 1, 2, 4, 2, 0, 0, 1, 4,
                   0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1])
</pre></div>


<p>Here, the <code>np</code> prefix indicates that the <code>array</code> function comes from the <a href="http://numpy.scipy.org/">Numpy</a> module. PyMC is able to recognize the presence of missing values when we use Numpy's MaskedArray class to contain our data. The masked array is instantiated via the <code>masked_array</code> function, using the original data array and a boolean mask as arguments: </p>
<div class="highlight"><pre>    masked_values = np.ma.masked_array(disasters_array,
    mask=disasters_array==-999)
</pre></div>


<p>Of course, my use of -999 to indicate missing data was entirely arbitrary, so feel free to use any appropriate value, so long as it can be identified and masked (obviously, small positive integers would not have been appropriate here). Let's have a look at the masked array:</p>
<div class="highlight"><pre>masked_array(data = [4 5 4 0 1 4 3 4 0 6 3 3 4 0 2 6 3 3 5 4 5 3 1 4 
    4 1 5 5 3 4 2 5 2 2 3 4 2 1 3 -- 2 1 1 1 1 3 0 0 1 0 1 1 0 0 3 1 
    0 3 2 2 0 1 1 1 0 1 0 1 0 0 0 2 1 0 0 0 1 1 0 2 3 3 1 -- 2 1 1 1 
    1 2 4 2 0 0 1 4 0 0 0 1 0 0 0 0 0 1 0 0 1 0 1], 
    mask = [False False False False False False False False False 
    False False False False False False False False False False False
    False False False False False False False False False False False
    False False False False False False False False True False False 
    False False False False False False False False False False False
    False False False False False False False False False False False
    False False False False False False False False False False False
    False False False False False False False False True False False 
    False False False False False False False False False False False
    False False False False False False False False False False False 
    False False False],
    fill_value = 999999)
</pre></div>


<p>Notice that the placeholder values have disappeared from the data, and the array has a <code>mask</code> attribute that identifies the indices for the missing values.</p>
<p>Beyond the construction of a masked array, there is nothing else that needs to be done to accommodate missing values in a PyMC model.</p>
<p>First, we need to specify prior distributions for the unknown parameters, which I call <code>switch</code> (the switch point), <code>early</code> (the early mean) and <code>late</code> (the late mean). An appropriate non-informative prior for the switch point is a discrete uniform random variable over the range of years represented by the data. Since the rates must be positive, I use identical weakly-informative exponential distributions:</p>
<div class="highlight"><pre># Switchpoint
switch = DiscreteUniform(&#39;switch&#39;, lower=0, upper=110)
# Early mean
early = Exponential(&#39;early&#39;, beta=1)
# Late mean
late = Exponential(&#39;late&#39;, beta=1)
</pre></div>


<p>The only tricky part of the model is assigning the appropriate rate parameter to each observation. Though the two rates and the switch point are stochastic, in the sense that we have used probability models to describe our uncertainty in their true values, the membership of each observation to either the early or late rate is a deterministic function of the stochastics. Thus, we set up a deterministic node that assigns a rate to each observation depending on the  location of the switch point at the current iteration of the MCMC algorithm:</p>
<div class="highlight"><pre>@deterministic
def rates(s=switch, e=early, l=late):
    &quot;&quot;&quot;Allocate appropriate mean to time series&quot;&quot;&quot;
    out = np.empty(len(disasters_array))
    # Early mean prior to switchpoint
    out[:s] = e
    # Late mean following switchpoint
    out[s:] = l
    return out
</pre></div>


<p>Finally, the data likelihood comprises the annual counts of disasters being modeled as Poisson random variables, conditional on the parameters assigned in the <code>rates</code> node above. The masked array is specified as the value of the stochastic node, and flagged as data via the <code>observed</code> argument.</p>
<div class="highlight"><pre>disasters = Poisson(&#39;disasters&#39;, mu=rates, value=masked_values, observed=True)
</pre></div>


<p>If we run the model, then query the <code>disasters</code> node for posterior statistics, we can obtain a summary of the estimated number of disasters in both of the missing years.</p>
<div class="highlight"><pre>In [9]: DisasterModel.disasters.stats()
Out[9]: 
{&#39;95% HPD interval&#39;: array([[ 0.,  6.],
       [ 0.,  3.]]),
 &#39;mc error&#39;: array([ 0.11645149,  0.03479713]),
 &#39;mean&#39;: array([ 2.2246,  0.91  ]),
 &#39;n&#39;: 5000,
 &#39;quantiles&#39;: {2.5: array([ 0.,  0.]),
               25: array([ 1.,  0.]),
               50: array([ 2.,  1.]),
               75: array([ 3.,  1.]),
               97.5: array([ 7.,  3.])},
 &#39;standard deviation&#39;: array([ 1.88206133,  0.92536479])}
</pre></div>


<p>Clearly, this is a rather trivial example, but it serves to illustrate how easy it can be to deal with missing values in PyMC. Though not applicable here, it would be similarly easy to handle MAR data, by constructing a data likelihood whose parameter(s) is a function of one or more covariates. </p>
<p>Automatic imputation is a new feature in PyMC, and is currently available only in the <a href="http://github.com/pymc-devs/pymc">development codebase</a>. It will hopefully appear in the feature set of a future release.</p>	

	</article>

    <p>
	<a href="https://twitter.com/share" class="twitter-share-button" data-via="fonnesbeck" data-lang="en" data-size="large" data-related="fonnesbeck">Tweet</a>
	<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src="//platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
	</p>


		  </div>	
		  
		  <div class="sidebar">
		    <div class="sidebar-container" id="sidebar-container-fixed">

	            <aside>
	              <h2>About</h2>
			      <p>
                    Bayes, Python and Data
			      </p>
			    </aside>

  	          <nav>
	            <h2>Categories</h2>
	            <ul>
	                <li class="active"><a href="http://stronginference.com/category/statistics.html">Statistics</a></li>
	            </ul>
	          </nav>

	            <aside>
	            <h2>Social</h2>
			      <ul class="social">
				    <li><a href="http://github.com/fonnesbeck">Github</a><i></i></li>
				    <li><a href="https://www.flickr.com/photos/fonnesbeck">flickr</a><i></i></li>
			      </ul>
			    </aside>

	            <aside>
	              <h2>Blogroll</h2>
	              <ul>
	                  <li><a href="http://andrewgelman.com/">Statistical Modeling, Causal Inference, and Social Science</a></li>
	                  <li><a href="http://www.johndcook.com/blog/">John D. Cook</a></li>
	                  <li><a href="https://jakevdp.github.io">Pythonic Perambulations</a></li>
	                  <li><a href="http://healthyalgorithms.com">Healthy Algorithms</a></li>
	              </ul>
	            </aside>
	        </div>
		  </div>

	  </div>

      <footer>
		<p role="contentinfo">
		  Christopher Fonnesbeck - Proudly powered by <a href="http://alexis.notmyidea.org/pelican/">pelican</a>. Theme <a href="https://github.com/fle/pelican-sober">pelican-sober</a>.
    	</p>

	  </footer>	

	</div>
	

</body>
</html>