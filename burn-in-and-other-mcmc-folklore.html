<!doctype html>
<html lang="">	
<head>
	<meta charset="utf-8"/>
	<title>Burn-in, and Other MCMC Folklore - Strong Inference</title>	
	<meta name="author" content="Christopher Fonnesbeck">
	

  <meta name="description" content="I have been slowly working my way through The Handbook of Markov Chain Monte Carlo, a compiled volume edited by Steve Brooks et al. that I picked up at last week's Joint Statistical Meetings ...">

	<meta name="twitter:card" content="summary">
	  <meta name="twitter:creator" content="@fonnesbeck">
	<meta name="twitter:title" content="Burn-in, and Other MCMC Folklore">
	<meta name="twitter:description" content="I have been slowly working my way through The Handbook of Markov Chain Monte Carlo, a compiled volume edited by Steve Brooks et al. that I picked up at last week's Joint Statistical Meetings ...">
	<meta name="twitter:url" content="http://stronginference.com/burn-in-and-other-mcmc-folklore.html">


	<link rel="stylesheet" href="http://stronginference.com/theme/css/main.css" type="text/css" />
		


</head>
	
<body>

    <div class="container">
	  
	  <header role="banner">
	    <div class="feeds">
	    </div>
		<a href="http://stronginference.com" class="title">Strong Inference</a>
      </header>
	
	  <div class="wrapper">

		  <div role="main" class="content">
	<article class="full">
		
		<h1>Burn-in, and Other MCMC Folklore</h1>
		
<div class="metadata">
  <time datetime="2014-08-09T00:00:00-05:00" pubdate>Sat 09 August 2014</time>
    <address class="vcard author">
      by <a class="url fn" href="http://stronginference.com/author/christopher-fonnesbeck.html">Christopher Fonnesbeck</a>
    </address>
  in <a href="http://stronginference.com/category/statistics.html">Statistics</a>
<p class="tags">tagged <a href="http://stronginference.com/tag/bayesian.html">bayesian</a>, <a href="http://stronginference.com/tag/mcmc.html">mcmc</a>, <a href="http://stronginference.com/tag/pymc.html">pymc</a>, <a href="http://stronginference.com/tag/python.html">python</a></p></div>		
		<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<p>I have been slowly working my way through <a href="http://amzn.to/mR9PVr">The Handbook of Markov Chain Monte Carlo</a>, a compiled volume edited by Steve Brooks <em>et al.</em> that I picked up at last week's Joint Statistical Meetings. The first chapter is a primer on MCMC by <a href="http://www.stat.umn.edu/~charlie/">Charles Geyer</a>, in which he summarizes the key concepts of the theory and application of MCMC. In a particularly provocative passage, Geyer rips several of the traditional practices in setting up, running and diagnosing MCMC runs, including multi-chain runs, burn-in and sample-based diagnostics. Though they are applied regularly, these steps are simply heuristics that are applied to either aid in reaching or identifying the equilibrium distribution of the Markov chain. There are no guarantees on the reliability of any of them.</p>
<p>In particular, he questions the utility of burn-in:</p>
<blockquote>
<p>Burn-in is only one method, and not a particuarly good method, for finding a good starting point.</p>
</blockquote>
<p>I can't disagree with this, though I have always viewed MCMC sampling (for most models that I have dealt with) as being cheap enough that there is little cost to simply throwing away thousands of them. I have often thrown away as many as the first 90 percent of my samples! However, as Geyer notes, there are better ways of getting your chain into a decent region of its support without throwing anything away.</p>
<p>One method is to use an approximation method on your model before applying MCMC. For example, the <a href="http://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation">maximum a posteriori (MAP)</a> estimate can be obtained using numerical optimization, then used as the initial values for an MCMC run. It turns out to be pretty easy to do in PyMC. For example, using the built-in bioassay example:</p>
<div class="highlight"><pre><span class="n">In</span> <span class="p">[</span><span class="mi">3</span><span class="p">]:</span> <span class="kn">from</span> <span class="nn">pymc.examples</span> <span class="kn">import</span> <span class="n">gelman_bioassay</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">4</span><span class="p">]:</span> <span class="kn">from</span> <span class="nn">pymc</span> <span class="kn">import</span> <span class="n">MAP</span><span class="p">,</span> <span class="n">MCMC</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">5</span><span class="p">]:</span> <span class="n">M</span> <span class="o">=</span> <span class="n">MAP</span><span class="p">(</span><span class="n">gelman_bioassay</span><span class="p">)</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">6</span><span class="p">]:</span> <span class="n">M</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>


<p>This yields MAP estimates for all the parameters in the model, which are less likely to be true modes as the complexity of the model increases, but are a pretty good bet to be a decent starting point for MCMC.</p>
<div class="highlight"><pre><span class="n">In</span> <span class="p">[</span><span class="mi">7</span><span class="p">]:</span> <span class="n">M</span><span class="o">.</span><span class="n">alpha</span><span class="o">.</span><span class="n">value</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">7</span><span class="p">]:</span> <span class="n">array</span><span class="p">(</span><span class="mf">0.8465802225061101</span><span class="p">)</span>
</pre></div>


<p>All that remains is to move these estimates into an MCMC sampler. While one could manually plug the values of each node into the model specification, its easiest just to extract the variables from the MAP estimator, and use them to instantiate an <code>MCMC</code> object:</p>
<div class="highlight"><pre><span class="n">In</span> <span class="p">[</span><span class="mi">8</span><span class="p">]:</span> <span class="n">M</span><span class="o">.</span><span class="n">variables</span>
<span class="n">Out</span><span class="p">[</span><span class="mi">8</span><span class="p">]:</span> 
<span class="nb">set</span><span class="p">([</span><span class="o">&lt;</span><span class="n">pymc</span><span class="o">.</span><span class="n">PyMCObjects</span><span class="o">.</span><span class="n">Stochastic</span> <span class="s">&#39;alpha&#39;</span> <span class="n">at</span> <span class="mh">0x10f78e810</span><span class="o">&gt;</span><span class="p">,</span>
     <span class="o">&lt;</span><span class="n">pymc</span><span class="o">.</span><span class="n">PyMCObjects</span><span class="o">.</span><span class="n">Stochastic</span> <span class="s">&#39;beta&#39;</span> <span class="n">at</span> <span class="mh">0x10f78e910</span><span class="o">&gt;</span><span class="p">,</span>
     <span class="o">&lt;</span><span class="n">pymc</span><span class="o">.</span><span class="n">PyMCObjects</span><span class="o">.</span><span class="n">Deterministic</span> <span class="s">&#39;theta&#39;</span> <span class="n">at</span> <span class="mh">0x10f78e9d0</span><span class="o">&gt;</span><span class="p">,</span>
     <span class="o">&lt;</span><span class="n">pymc</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Binomial</span> <span class="s">&#39;deaths&#39;</span> <span class="n">at</span> <span class="mh">0x10f78ea50</span><span class="o">&gt;</span><span class="p">,</span>
     <span class="o">&lt;</span><span class="n">pymc</span><span class="o">.</span><span class="n">CommonDeterministics</span><span class="o">.</span><span class="n">Lambda</span> <span class="s">&#39;LD50&#39;</span> <span class="n">at</span> <span class="mh">0x10f78ec10</span><span class="o">&gt;</span><span class="p">])</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">9</span><span class="p">]:</span> <span class="n">MC</span> <span class="o">=</span> <span class="n">MCMC</span><span class="p">(</span><span class="n">M</span><span class="o">.</span><span class="n">variables</span><span class="p">)</span>

<span class="n">In</span> <span class="p">[</span><span class="mi">10</span><span class="p">]:</span> <span class="n">MC</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">Sampling</span><span class="p">:</span> <span class="mi">100</span><span class="o">%</span> <span class="p">[</span><span class="mo">0000000000000000000000000000000000000000000000</span><span class="p">]</span> <span class="n">Iterations</span><span class="p">:</span> <span class="mi">1000</span>
</pre></div>


<p>Notice that I did not pass a <code>burn</code> argument to MCMC, which defaults to zero. As is evident from the graphical output of the posteriors, this results in what appears to be a homogeneous chain, and which is hopefully already at its equilibrium distribution.</p>
<p><img src="http://f.cl.ly/items/4513263v3x3n1T0m3o27/alpha.png" width="500"></p>
<p><img src="http://f.cl.ly/items/1i0W0k1Q2S3h172E2v0b/beta.png" width="500"></p>
<p>What the MCMC practitioner fears is using a chain for inference that has not yet converged to its target distribution. Unfortunately, diagnostics cannot reliably alert you to this, nor does starting a model in several chains from disparate starting values guarantee this. There is also no magical threshold to distinguish convergence from pre-convergence regions in a MCMC trace. Geyer insists that only running chains for a very, very long time will inspire confidence:</p>
<blockquote>
<p>Your humble author has a dictum that the lease one can do is make an overnight run. ... If you do not make runs like that, you are simply not serious about MCMC.</p>
</blockquote>	

	</article>

    <p>
	<a href="https://twitter.com/share" class="twitter-share-button" data-via="fonnesbeck" data-lang="en" data-size="large" data-related="fonnesbeck">Tweet</a>
	<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src="//platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
	</p>


		  </div>	
		  
		  <div class="sidebar">
		    <div class="sidebar-container" >


  	          <nav>
	            <h2>Categories</h2>
	            <ul>
	                <li class="active"><a href="http://stronginference.com/category/statistics.html">Statistics</a></li>
	            </ul>
	          </nav>

	            <aside>
	            <h2>Social</h2>
			      <ul class="social">
				    <li><a href="https://twitter.com/fonnesbeck">Twitter</a><i></i></li>
				    <li><a href="http://github.com/fonnesbeck">Github</a><i></i></li>
			      </ul>
			    </aside>

	            <aside>
	              <h2>Blogroll</h2>
	              <ul>
	                  <li><a href="http://andrewgelman.com/">Statistical Modeling, Causal Inference, and Social Science</a></li>
	                  <li><a href="http://www.johndcook.com/blog/">John D. Cook</a></li>
	                  <li><a href="https://jakevdp.github.io">Pythonic Perambulations</a></li>
	                  <li><a href="http://healthyalgorithms.com">Healthy Algorithms</a></li>
	                  <li><a href="http://glau.ca">Glau.ca</a></li>
	                  <li><a href="http://statsinthewild.com">Stats in the Wild</a></li>
	                  <li><a href="http://simplystatistics.org">SimplyStats</a></li>
	                  <li><a href="http://blog.yhathq.com">Å·hat</a></li>
	              </ul>
	            </aside>
	        </div>
		  </div>

	  </div>

      <footer>
		<p role="contentinfo">
		  Christopher Fonnesbeck - Proudly powered by <a href="http://alexis.notmyidea.org/pelican/">pelican</a>. Theme <a href="https://github.com/fle/pelican-sober">pelican-sober</a>.
    	</p>

	  </footer>	

	</div>
	

</body>
</html>